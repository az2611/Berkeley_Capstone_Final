# CLAIMS DESCRIPTION CLASSIFICATION WITH MACHINE LEARNING AND LARGE LANGUAGE MODELS
Alejandro Zarate M.Sc. MBA.  
March 2023, New York, New York. 

## EXECUTIVE SUMMARY
Claims processing in insurance refers to the procedures and activities involved in receiving, verifying, and resolving claims filed by policyholders. This process typically includes reviewing the claim details, assessing coverage, investigating the circumstances surrounding the claim, and determining the amount of compensation to be paid.  
In most cases, claims descriptions present as open text, requiring classifying these into categories as it allows for easier organization and analysis. However, this is a challenging and time-consuming task often performed by hand, and requires specialized knowledge.  
We hypothesize we can automate the classification of claims descriptions. For this, we have evaluated a set of anonymized casualty loss descriptions within nine categories using three traditional machine-learning classification methods (Logistic regression, Decision tree, Naïve Bayes) and two pre-trained large language models (BERT, GPT-3) achieving accuracy above 90%.  
Initial results reported here are encouraging and should be considered exploratory, we suggest expanding work to refine these methods and develop models in more risk practices and specialties.  

## INTRODUCTION
Often, claims descriptions present as open text, requiring classifying these into predefined categories as it allows for easier organization and analysis. However, this is a time-consuming task often performed by hand, and requires specialized knowledge. Table 1 presents five examples extracted from the dataset used.

*Table 1. Extract of open text claims description and corresponding classification*  
|Description|Class|
|---|---|
|CLAIMANT FELL AND INJURED WRIST|Fell|
|FLOODING OF PREMISES|Flood|
|FELL IN HOLE & INJURED LEG & KNEE|Fell|
|STRUCK BY WIRE ON TRAIN PLATFORM SERIOUS BACK INJURY|Injury|
|OIL SPILLAGE|Pollution|

Machine Learning and pre-trained can be a valuable tool for claims description classification in the insurance industry. By using pre-trained models, insurance companies can leverage the vast amounts of text data generated by claims to train more accurate and efficient models that can handle complex and diverse language patterns.  
Pre-trained models, such as BERT and GPT-3, have already demonstrated their effectiveness in various NLP applications, and their use in claims classification could lead to faster and more accurate processing of claims data, reducing manual effort, and improving decision-making.  

### Text Classification
Text classification is a natural language processing (NLP) technique used to categorize text documents into predefined categories or classes. It involves analyzing the content of the text to identify relevant features and then using machine-learning algorithms to train a model that can accurately predict the category of new, unseen text. An example of text classification is sentiment analysis, a use case for text classification commonly used in social media monitoring, brand reputation management, and customer feedback analysis to gauge public opinion and sentiment towards a product, service, or topic.  
### Traditional Machine Learning
Logistic regression, naive Bayes, and decision trees are popular machine-learning algorithms used for multi-class text classification.  
* Logistic regression is a linear model that estimates the probability of a text belonging to a certain class by fitting a logistic function to the training data.  
* Naive Bayes is a probabilistic model that uses Bayes' theorem to calculate the probability of a text belonging to a certain class based on the occurrence of certain features in the text. 
* Decision trees, on the other hand, are non-parametric models that use a tree-like structure to classify texts based on the features in the text. 
Pre-trained Large Language Models (LLM)
Transfer learning is a machine learning technique that involves reusing pre-trained models on a new task or problem domain.  Pre-trained models are pre-built machine learning models that previously trained on large datasets, allowing the transfer of knowledge to the new task. Fine-tuning pre-trained models in NLP, enables faster training and better performance with smaller data sets. 
* BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google that learns contextual relationships between words in a sentence. It can be fine-tuned for text classification by adding a task-specific layer on top of the pre-trained model.
* GPT-3 is a transformer-based language model developed by OpenAI with 175 billion parameters. It has been pre-trained on a large amount of text data, enabling it to generate human-like text in response to various prompts. GPT-3 can perform tasks such as language translation, text summarization, question-answering, and creative writing. 

## RESEARCH QUESTIONS
We aim to answer the following research questions:  
* What is the impact of different pre-processing techniques, such as stemming and stop-word removal, on the performance of pre-trained models in multi-class text classification tasks?  
* How do traditional techniques, and pre-trained models, such as BERT and GPT, compare in terms of performance when applied to multi-class text classification tasks?  
* What is the time and cost associated with testing traditional and pre-trained models?  

## THE DATASET
Data was obtained by collating a random sample of text descriptions for +2200 anonymized claims classified in nine categories (Table 2.) No traceable personal information is included in the dataset.  

*Table 2. Claims Categories.*  
| Description              | No. of Samples |
| ----------------------- | ------------- |
| Auto-related             | 233           |
| Damage                   | 192           |
| Fell                     | 509           |
| Fire                     | 44            |
| Flood                    | 62            |
| Injury                   | 842           |
| Occupational disease/illness | 146       |
| Pollution                | 62            |
| Water                    | 157           |

*Figure 1. Class Frequency*

![image](https://user-images.githubusercontent.com/108035242/224762239-b5229e1c-3165-4a90-b13f-4c7744d95693.png)
  
## EXPERIMENTAL SETUP
The following presents a high-level description of the experiment al set up follow to to apply pre-trained and machine-learning models to multi-class text classification of claims data: 
1. Data pre-processing and cleaning, including removing stop words, stemming, and tokenization.
2. Data split into training and validation sets.
3. Model training and fine-tuning for the five models were selected:
   - Logistic Regression
   - Naïve Bayes
   - Decision Trees
   - BERT
   - GPT-3
4. Comparing the performance among models.


## EXPERIMENTAL RESULTS
For the purpose of this exploration, we use accuracy as main performance criteria. Accuracy is a common evaluation metric used in machine learning that measures how well a machine-learning model is able to correctly predict the outcome or class of a given input by calculation the percentage of the total predictions that the model got correct. High accuracy values indicate that the model is making correct predictions more frequently.  
For BERT and GPT-3 we also report f1-score. F1-score is a metric used to evaluate the performance of a classification model in machine learning. The F1-score provides a single score that represents the overall performance of the model. It ranges between 0 and 1, with a higher score indicating better performance.  

### Traditional Machine Learning  
By comparing model performance in terms of accuracy and speed, we can see that Logistic Regression provides an accuracy score on 0.8637 with a training time of about 12 seconds.  

*Table 3. Traditional Machine Learning Results*
| Model                   | Stemming               | Vectorizer | Best Max Features | Stop Words | Accuracy | Time  |
| -----------------------| ---------------------- | ---------- | ---------------- | ---------- | -------- | ----- |
| LogisticRegression_l2   | PorterStemmer()        | cvect      | 500              |            | 0.8637   | 12.37 |
| LogisticRegression_l2   | WordNetLemmatizer()    | cvect      | 500              |            | 0.8548   | 16.09 |
| LogisticRegression_l2   | WordNetLemmatizer()    | tdif       | 500              |            | 0.8370   | 16.62 |
| DecisionTree            | PorterStemmer()        | cvect      | 2000             |            | 0.8355   | 2.04  |
| LogisticRegression_l2   | PorterStemmer()        | tdif       | 500              |            | 0.8340   | 12.5  |
| DecisionTree            | WordNetLemmatizer()    | tdif       | 1000             |            | 0.8192   | 2.92  |
| DecisionTree            | WordNetLemmatizer()    | cvect      | 3000             |            | 0.8148   | 3.22  |
| DecisionTree            | PorterStemmer()        | tdif       | 500              |            | 0.8118   | 2.75  |
| NaiveBayes              | WordNetLemmatizer()    | cvect      | 500              | english    | 0.8      | 1.22  |
| NaiveBayes              | PorterStemmer()        | cvect      | 500              |            | 0.7970   | 1.22  |
| NaiveBayes              | WordNetLemmatizer()    | tdif       | 500              | english    | 0.7822   | 1.34  |
| NaiveBayes              | PorterStemmer()        | tdif       | 100              | english    | 0.7762   | 1.31  |

*Figure 2. Training time*

![image](https://user-images.githubusercontent.com/108035242/224762744-eecb9269-4ef3-4c5e-b65c-add40ae1f5fa.png)


### BERT
We can see that the BERT approach produces better results when we compare it to Logistic Regression. Note that when we review the results on the test set, we observe some mistakes in the classifications performed by hand. For the second iteration of this project, we will fix these errors, and we expect an improvement in accuracy with this model.  

Accuracy: 0.91239		f1 score: 0.894  

### GPT-3
For the GPT approach, we evaluated two different GPT-3 models: ADA and DaVinci.  
ADA is a smaller, more specialized version of GPT-3 that is designed for use in specific industries or applications, such as healthcare or finance. It has fewer parameters than the larger GPT-3 models, which allows it to be fine-tuned for specific tasks more efficiently. Davinci, on the other hand, is the largest and most powerful version of GPT-3. It has 175 billion parameters.  
Is important to note that neither model produces better results when we compare it to BERT. However, is important to note that further research with a more robust data set needs to be perform to confirm these results. I both cases training time was well over 30 minutes.  

*Table 4. GPT-3 results for claims text classification*
|      | Accuracy | f1 score |
| ---- | -------- | -------- |
| ADA  | 0.8689   | 0.8687   |
| DaVinci | 0.8822 | 0.8819   |

*Figure 3. Accuracy for DaVinci Model*  

<img src="https://user-images.githubusercontent.com/108035242/224757100-3c77eb86-b9b8-40eb-89e3-97383a56b62c.png" alt="image" width="70%">


## CONCLUSIONS AND FUTURE WORK
From this initial work, we can initially conclude that bigger not necessarily means better, as we can see in our initial results traditional machine-learning technique or medium-size language models can achieve acceptable levels of performance without incurring with the expense associated with using larger models like GPT-3.  


*Figure 4. Results Comparison*  

<img width="800" alt="image" src="https://user-images.githubusercontent.com/108035242/224755990-e7b6be3c-fa31-47b1-a059-fb7cbaf47bfd.png">

The initial evidence suggests that we can achieve automated claims text classification. Improving model optimization, data quality, quantity, and balancing classes can positively contribute to improving performance.  Computational and financial cost of using larger models (GPT-3) may not always reflect in better performance. It is important to keep in mind that model selection is essential to achieve optimal economics and processing time.  

## Next Steps
* Develop a more robust and balanced dataset, reevaluate the techniques and compare results.
* Explore the generative potential of BERT and GPT-3 to assist in the analysis of claims data.
* Explore the generative potential of BERT and GPT-3 to assist in claims description writing.


